{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "eHQPPTOhTmuq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_hub\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDEzIpmzYLU3",
        "outputId": "7ebf33d0-a5c1-49d4-ebc1-bcbfa24aaa0b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow_hub in /usr/local/lib/python3.10/dist-packages (0.13.0)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_hub) (1.22.4)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow_hub) (3.20.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "data = pd.read_csv('ruddit_comments_score.csv')\n",
        "\n",
        "# Preprocess the data if required (e.g., removing unnecessary columns, cleaning text, etc.)\n",
        "\n",
        "# Print the column names to verify the correct column name\n",
        "print(data.columns)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EavjuBQwTtuv",
        "outputId": "6dc2b9e4-aa05-4941-82a7-597d3d75adbd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['comment_id', 'body', 'score'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Text Preprocessing\n",
        "\n",
        "# Example text preprocessing steps\n",
        "data['body'] = data['body'].str.lower()  # Convert text to lowercase\n",
        "# Apply other preprocessing steps as needed\n"
      ],
      "metadata": {
        "id": "OyHEmu1oUmju"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Train a Chatbot Model (Retrieval-based approach using Universal Sentence Encoder)\n",
        "\n",
        "# Load the pre-trained Universal Sentence Encoder model\n",
        "use_model = hub.load('https://tfhub.dev/google/universal-sentence-encoder-large/5')\n",
        "\n",
        "# Encode the comments using the Universal Sentence Encoder in batches\n",
        "batch_size = 1000\n",
        "encoded_comments = []\n",
        "for i in range(0, len(data), batch_size):\n",
        "    batch_comments = data['body'].iloc[i:i+batch_size].values\n",
        "    encoded_batch = use_model(batch_comments)\n",
        "    encoded_comments.append(encoded_batch)\n",
        "\n",
        "encoded_comments = np.concatenate(encoded_comments)\n"
      ],
      "metadata": {
        "id": "MsyfqMFGT4GR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Build the Chatbot Interface\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"User: \")\n",
        "\n",
        "    # Perform any necessary preprocessing on the user input\n",
        "    user_input = user_input.lower()\n",
        "\n",
        "    # Encode the user input using the Universal Sentence Encoder\n",
        "    encoded_user_input = use_model([user_input])\n",
        "\n",
        "    # Compute similarity scores between the user input and the encoded comments\n",
        "    similarity_scores = np.dot(encoded_comments, np.transpose(encoded_user_input))\n",
        "\n",
        "    # Find the index of the most similar comment\n",
        "    most_similar_index = np.argmax(similarity_scores)\n",
        "\n",
        "    # Get the corresponding response from the dataset\n",
        "    response = data['body'].iloc[most_similar_index]\n",
        "\n",
        "    print(\"Chatbot: \" + response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "is_yxDlfT9UI",
        "outputId": "b7bc7cc8-448f-4924-8813-7bd500c24019"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: hi\n",
            "Chatbot: [deleted]\n",
            "User: hello\n",
            "Chatbot: get the fuck out of here.\n",
            "User: oh\n",
            "Chatbot: wait whoa whoa whoa what the fuck what the fuck \n",
            "User: bruh stop this shit\n",
            "Chatbot: this shit is getting wild.\n",
            "User: you might be my greatest creation\n",
            "Chatbot: you’re a fucking legend \n",
            "User: damn right\n",
            "Chatbot: yep. heck of a way to make a point. \n",
            "User: nigga u have no rights \n",
            "Chatbot: youre under citizen's arrest\n",
            "User: but i am the white one here\n",
            "Chatbot: yes i look white anyways\n",
            "User: how bruh you blacker than my balls \n",
            "Chatbot: i wonder if you’d have the same tone if it was a dude’s hairy taint instead of a pussy.\n"
          ]
        }
      ]
    }
  ]
}